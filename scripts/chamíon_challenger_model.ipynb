{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ef5152a",
   "metadata": {},
   "source": [
    "# Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "409c5c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import preprocessing\n",
    "from pathlib import Path\n",
    "from sklearn.kernel_approximation import Nystroem\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score\n",
    "from category_encoders import TargetEncoder\n",
    "import xgboost as xgb\n",
    "import mlflow\n",
    "import mlflow.pyfunc\n",
    "from sklearn.metrics import recall_score, precision_score, roc_auc_score\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fd831e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = \"../data/platinum/\"\n",
    "output_path = \"../data/platinum/\"\n",
    "\n",
    "Path(output_path).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94325e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = joblib.load(input_path + \"xgb/model.pkl\")\n",
    "svm_model = joblib.load(input_path + \"svm/model.pkl\")\n",
    "lstm_model = joblib.load(input_path + \"lstm/model.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462de656",
   "metadata": {},
   "source": [
    "# Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62706200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 1749494\n",
      "Training rows: 1574544 (90%)\n",
      "Testing rows: 174950 (10%)\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df = train_test_split(all_data, test_size=0.10, random_state=42, stratify=all_data['anomaly'])\n",
    "\n",
    "# Verify the results\n",
    "print(f\"Total rows: {len(all_data)}\")\n",
    "print(f\"Training rows: {len(train_df)} (90%)\")\n",
    "print(f\"Testing rows: {len(test_df)} (10%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de6ef419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Training Set Size: 111886\n",
      "Class 1 (Anomaly): 33566 (30.0%)\n",
      "Class B (Normal): 78320 (70.0%)\n"
     ]
    }
   ],
   "source": [
    "df_anomaly = train_df[train_df['anomaly'] == 1] # The 40,000 anomalies\n",
    "df_normal = train_df[train_df['anomaly'] == 0]  # The 1.7M normal cases\n",
    "\n",
    "# 2. Calculate the required number of normal samples for a 70/30 split\n",
    "# If 40k is 30%, then X is 70% -> (40,000 / 0.3) * 0.7\n",
    "num_normal_needed = int((len(df_anomaly) / 0.3) * 0.7)\n",
    "\n",
    "# 3. Downsample Class B (Normal)\n",
    "df_normal_downsampled = df_normal.sample(n=num_normal_needed, random_state=42)\n",
    "\n",
    "# 4. Combine them into your new training set\n",
    "df_train_7030 = pd.concat([df_anomaly, df_normal_downsampled])\n",
    "\n",
    "# 5. Shuffle the resulting dataframe\n",
    "df_train_7030 = df_train_7030.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Verification\n",
    "print(f\"New Training Set Size: {len(df_train_7030)}\")\n",
    "print(f\"Class 1 (Anomaly): {len(df_anomaly)} ({len(df_anomaly)/len(df_train_7030):.1%})\")\n",
    "print(f\"Class B (Normal): {len(df_normal_downsampled)} ({len(df_normal_downsampled)/len(df_train_7030):.1%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830cbc10",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3986a2aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of records droped for having NaN values: 0.11%\n"
     ]
    }
   ],
   "source": [
    "train_svm = df_train_7030.dropna(inplace=False)\n",
    "test_svm = test_df.dropna(inplace=False)\n",
    "print(f\"% of records droped for having NaN values: {(1 - (len(train_svm) / len(df_train_7030))) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0e2584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.79      0.88    171015\n",
      "           1       0.04      0.44      0.08      3728\n",
      "\n",
      "    accuracy                           0.78    174743\n",
      "   macro avg       0.51      0.61      0.48    174743\n",
      "weighted avg       0.96      0.78      0.86    174743\n",
      "\n",
      "Accuracy Score: 0.7811\n",
      "ROC AUC Score: 0.6544\n"
     ]
    }
   ],
   "source": [
    "# 1. Prepare the data (handling the numeric selection as you did)\n",
    "X = train_svm.select_dtypes(include=['int64', 'float64']).drop(['anomaly', 'timestamp'], axis=1, errors='ignore')\n",
    "y = train_svm['anomaly']\n",
    "\n",
    "# 2. Define the Nystroem transformer \n",
    "# n_components=300 is a good balance between speed and accuracy. \n",
    "# Increasing this improves accuracy but slows down training.\n",
    "nystroem_stage = Nystroem(kernel='poly', degree=3, coef0=1, n_components=300, random_state=42)\n",
    "\n",
    "# 3. Use a fast linear solver\n",
    "# SGDClassifier is optimized for large datasets (100k+ rows)\n",
    "clf_stage = SGDClassifier(\n",
    "    loss='hinge', \n",
    "    class_weight='balanced',  # <--- THE MAGIC FIX\n",
    "    alpha=0.01, \n",
    "    max_iter=1000, \n",
    "    tol=1e-3,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 2. Re-build the same pipeline structure\n",
    "poly_svm_balanced = Pipeline([\n",
    "    ('scaler', preprocessing.StandardScaler()),\n",
    "    ('nystroem', nystroem_stage),\n",
    "    ('clf', clf_stage)\n",
    "])\n",
    "\n",
    "# 3. Re-train (This will still be fast!)\n",
    "poly_svm_balanced.fit(X, y)\n",
    "\n",
    "# 4. test\n",
    "X_test = test_svm.select_dtypes(include=['int64', 'float64']).drop(['anomaly'], axis=1, errors='ignore')\n",
    "y_test = test_svm['anomaly']\n",
    "y_pred = poly_svm_balanced.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(f\"Accuracy Score: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "\n",
    "y_scores = poly_svm_balanced.decision_function(X_test)\n",
    "print(f\"ROC AUC Score: {roc_auc_score(y_test, y_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5830d856",
   "metadata": {},
   "source": [
    "## XG Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "49bf03a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train_7030.drop(['anomaly','timestamp'], axis=1)\n",
    "y_train = df_train_7030['anomaly']\n",
    "\n",
    "X_test = test_df.drop(['anomaly','timestamp'], axis=1)\n",
    "y_test = test_df['anomaly']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e57defbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters\n",
    "num_neg = (y_train == 0).sum()\n",
    "num_pos = (y_train == 1).sum()\n",
    "scale_weight = num_neg / num_pos\n",
    "\n",
    "xgb_params = { \\\n",
    "    'n_estimators' : 100,\n",
    "    'max_depth' : 6,\n",
    "    'learning_rate' : 0.1,\n",
    "    'scale_pos_weight' : scale_weight,\n",
    "    'tree_method' : 'hist',      \n",
    "    'enable_categorical' : True, \n",
    "    'random_state' : 42,\n",
    "    'use_label_encoder' : False,\n",
    "    'eval_metric' : 'logloss'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3950bd0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ferna\\Documents\\Repos\\e2e_ml_pipeline\\.venv\\Lib\\site-packages\\xgboost\\training.py:200: UserWarning: [00:23:57] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:782: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Training Complete!\n"
     ]
    }
   ],
   "source": [
    "# 1. Identify categorical columns\n",
    "cat_cols = ['building_month', 'building_hour', 'building_week_day_hour', 'primary_use']\n",
    "\n",
    "# 2. Instantiate and Fit the Encoder\n",
    "# 'smoothing' helps when a category only appears a few times\n",
    "encoder = TargetEncoder(cols=cat_cols, smoothing=1.0)\n",
    "\n",
    "# 3. Transform your data\n",
    "# This converts the strings into 'anomaly probability' floats\n",
    "X_train_encoded = encoder.fit_transform(X_train, y_train)\n",
    "X_test_encoded = encoder.transform(X_test)\n",
    "\n",
    "num_neg = (y == 0).sum()\n",
    "num_pos = (y == 1).sum()\n",
    "scale_weight = num_neg / num_pos\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators= xgb_params['n_estimators'],\n",
    "    max_depth= xgb_params['max_depth'],\n",
    "    learning_rate= xgb_params['learning_rate'],\n",
    "    scale_pos_weight= xgb_params['scale_pos_weight'], # From our previous calculation\n",
    "    tree_method= xgb_params['tree_method'],      \n",
    "    enable_categorical= xgb_params['enable_categorical'], \n",
    "    random_state= xgb_params['random_state'],\n",
    "    use_label_encoder= xgb_params['use_label_encoder'],\n",
    "    eval_metric= xgb_params['eval_metric']\n",
    ")\n",
    "\n",
    "# 4. Fit the model\n",
    "# On 100k rows, this should take anywhere from 10 to 60 seconds\n",
    "xgb_model.fit(X_train_encoded, y_train)\n",
    "\n",
    "print(\"Model Training Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "635d5068",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_xgb = X_test_encoded\n",
    "y_pred_xgb = xgb_model.predict(X_test_xgb)\n",
    "y_probs_xgb = xgb_model.predict_proba(X_test_xgb)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee37d7c",
   "metadata": {},
   "source": [
    "# Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6eec573",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "92b90a7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/platinum/svm/model.pkl']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(poly_svm_balanced, output_path + \"svm/model.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9957e1",
   "metadata": {},
   "source": [
    "## XG-Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321ac8c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/platinum/xgb/model.pkl']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(xgb_model, output_path + \"xgb/model.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec655fd",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e29fb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
