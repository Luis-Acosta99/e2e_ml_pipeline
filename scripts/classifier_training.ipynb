{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ef5152a",
   "metadata": {},
   "source": [
    "# Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409c5c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import preprocessing\n",
    "from pathlib import Path\n",
    "from sklearn.kernel_approximation import Nystroem\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score\n",
    "from category_encoders import TargetEncoder\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8fd831e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = \"../data/gold/\"\n",
    "output_path = \"../data/platinum/\"\n",
    "\n",
    "Path(output_path).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94325e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.read_parquet(input_path + \"anomaly_features.parquet\", engine='pyarrow')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462de656",
   "metadata": {},
   "source": [
    "# Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62706200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 1749494\n",
      "Training rows: 1574544 (90%)\n",
      "Testing rows: 174950 (10%)\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df = train_test_split(all_data, test_size=0.10, random_state=42, stratify=all_data['anomaly'])\n",
    "\n",
    "# Verify the results\n",
    "print(f\"Total rows: {len(all_data)}\")\n",
    "print(f\"Training rows: {len(train_df)} (90%)\")\n",
    "print(f\"Testing rows: {len(test_df)} (10%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de6ef419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Training Set Size: 111886\n",
      "Class 1 (Anomaly): 33566 (30.0%)\n",
      "Class B (Normal): 78320 (70.0%)\n"
     ]
    }
   ],
   "source": [
    "df_anomaly = train_df[train_df['anomaly'] == 1] # The 40,000 anomalies\n",
    "df_normal = train_df[train_df['anomaly'] == 0]  # The 1.7M normal cases\n",
    "\n",
    "# 2. Calculate the required number of normal samples for a 70/30 split\n",
    "# If 40k is 30%, then X is 70% -> (40,000 / 0.3) * 0.7\n",
    "num_normal_needed = int((len(df_anomaly) / 0.3) * 0.7)\n",
    "\n",
    "# 3. Downsample Class B (Normal)\n",
    "df_normal_downsampled = df_normal.sample(n=num_normal_needed, random_state=42)\n",
    "\n",
    "# 4. Combine them into your new training set\n",
    "df_train_7030 = pd.concat([df_anomaly, df_normal_downsampled])\n",
    "\n",
    "# 5. Shuffle the resulting dataframe\n",
    "df_train_7030 = df_train_7030.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Verification\n",
    "print(f\"New Training Set Size: {len(df_train_7030)}\")\n",
    "print(f\"Class 1 (Anomaly): {len(df_anomaly)} ({len(df_anomaly)/len(df_train_7030):.1%})\")\n",
    "print(f\"Class B (Normal): {len(df_normal_downsampled)} ({len(df_normal_downsampled)/len(df_train_7030):.1%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830cbc10",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3986a2aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of records droped for having NaN values: 0.11%\n"
     ]
    }
   ],
   "source": [
    "train_svm = df_train_7030.dropna(inplace=False)\n",
    "test_svm = test_df.dropna(inplace=False)\n",
    "print(f\"% of records droped for having NaN values: {(1 - (len(train_svm) / len(df_train_7030))) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0e2584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.79      0.88    171015\n",
      "           1       0.04      0.44      0.08      3728\n",
      "\n",
      "    accuracy                           0.78    174743\n",
      "   macro avg       0.51      0.61      0.48    174743\n",
      "weighted avg       0.96      0.78      0.86    174743\n",
      "\n",
      "Accuracy Score: 0.7811\n",
      "ROC AUC Score: 0.6544\n"
     ]
    }
   ],
   "source": [
    "# 1. Prepare the data (handling the numeric selection as you did)\n",
    "X = train_svm.select_dtypes(include=['int64', 'float64']).drop(['anomaly', 'timestamp'], axis=1, errors='ignore')\n",
    "y = train_svm['anomaly']\n",
    "\n",
    "# 2. Define the Nystroem transformer \n",
    "# n_components=300 is a good balance between speed and accuracy. \n",
    "# Increasing this improves accuracy but slows down training.\n",
    "nystroem_stage = Nystroem(kernel='poly', degree=3, coef0=1, n_components=300, random_state=42)\n",
    "\n",
    "# 3. Use a fast linear solver\n",
    "# SGDClassifier is optimized for large datasets (100k+ rows)\n",
    "clf_stage = SGDClassifier(\n",
    "    loss='hinge', \n",
    "    class_weight='balanced',  # <--- THE MAGIC FIX\n",
    "    alpha=0.01, \n",
    "    max_iter=1000, \n",
    "    tol=1e-3,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 2. Re-build the same pipeline structure\n",
    "poly_svm_balanced = Pipeline([\n",
    "    ('scaler', preprocessing.StandardScaler()),\n",
    "    ('nystroem', nystroem_stage),\n",
    "    ('clf', clf_stage)\n",
    "])\n",
    "\n",
    "# 3. Re-train (This will still be fast!)\n",
    "poly_svm_balanced.fit(X, y)\n",
    "\n",
    "# 4. test\n",
    "X_test = test_svm.select_dtypes(include=['int64', 'float64']).drop(['anomaly'], axis=1, errors='ignore')\n",
    "y_test = test_svm['anomaly']\n",
    "y_pred = poly_svm_balanced.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(f\"Accuracy Score: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "\n",
    "y_scores = poly_svm_balanced.decision_function(X_test)\n",
    "print(f\"ROC AUC Score: {roc_auc_score(y_test, y_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5830d856",
   "metadata": {},
   "source": [
    "## XG Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bf03a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_df\n",
    "#df_train_7030\n",
    "X_train = df_train_7030.drop(['anomaly','timestamp'], axis=1)\n",
    "y_train = df_train_7030['anomaly']\n",
    "\n",
    "X_test = test_df.drop(['anomaly','timestamp'], axis=1)\n",
    "y_test = test_df['anomaly']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3950bd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Identify categorical columns\n",
    "cat_cols = ['building_month', 'building_hour', 'building_week_day_hour', 'primary_use']\n",
    "\n",
    "# 2. Instantiate and Fit the Encoder\n",
    "# 'smoothing' helps when a category only appears a few times\n",
    "encoder = TargetEncoder(cols=cat_cols, smoothing=1.0)\n",
    "\n",
    "# 3. Transform your data\n",
    "# This converts the strings into 'anomaly probability' floats\n",
    "X_train_encoded = encoder.fit_transform(X_train, y_train)\n",
    "X_test_encoded = encoder.transform(X_test)\n",
    "\n",
    "num_neg = (y == 0).sum()\n",
    "num_pos = (y == 1).sum()\n",
    "scale_weight = num_neg / num_pos\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    scale_pos_weight=scale_weight, # From our previous calculation\n",
    "    tree_method='hist',      \n",
    "    enable_categorical=True, \n",
    "    random_state=42,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "\n",
    "# 4. Fit the model\n",
    "# On 100k rows, this should take anywhere from 10 to 60 seconds\n",
    "xgb_model.fit(X_train_encoded, y_train)\n",
    "\n",
    "print(\"Model Training Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee37d7c",
   "metadata": {},
   "source": [
    "# Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fd808735",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.to_parquet(output_path + \"anomaly_features.parquet\", engine='pyarrow', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
