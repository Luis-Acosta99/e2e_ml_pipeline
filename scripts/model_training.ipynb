{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ef5152a",
   "metadata": {},
   "source": [
    "# Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "409c5c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import preprocessing\n",
    "from pathlib import Path\n",
    "from sklearn.kernel_approximation import Nystroem\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score\n",
    "from category_encoders import TargetEncoder\n",
    "import xgboost as xgb\n",
    "import mlflow\n",
    "import mlflow.pyfunc\n",
    "from sklearn.metrics import recall_score, precision_score, roc_auc_score\n",
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8fd831e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = \"../data/gold/\"\n",
    "output_path = \"../data/platinum/models/\"\n",
    "output_path_shap = \"../data/platinum/shap/\"\n",
    "\n",
    "Path(output_path).mkdir(parents=True, exist_ok=True)\n",
    "Path(output_path_shap).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94325e14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of records droped for having NaN values: 0.12%\n"
     ]
    }
   ],
   "source": [
    "raw_data = pd.read_parquet(input_path + \"anomaly_features.parquet\", engine='pyarrow')\n",
    "all_data = raw_data.dropna(inplace=False)\n",
    "print(f\"% of records droped for having NaN values: {(1 - (len(all_data) / len(raw_data))) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462de656",
   "metadata": {},
   "source": [
    "# Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62706200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 1747348\n",
      "Training rows: 1572613 (90%)\n",
      "Testing rows: 174735 (10%)\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df = train_test_split(all_data, test_size=0.10, random_state=42, stratify=all_data['anomaly'])\n",
    "\n",
    "# Verify the results\n",
    "print(f\"Total rows: {len(all_data)}\")\n",
    "print(f\"Training rows: {len(train_df)} (90%)\")\n",
    "print(f\"Testing rows: {len(test_df)} (10%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de6ef419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Training Set Size: 111813\n",
      "Class 1 (Anomaly): 33544 (30.0%)\n",
      "Class B (Normal): 78269 (70.0%)\n"
     ]
    }
   ],
   "source": [
    "df_anomaly = train_df[train_df['anomaly'] == 1] # The 40,000 anomalies\n",
    "df_normal = train_df[train_df['anomaly'] == 0]  # The 1.7M normal cases\n",
    "\n",
    "# 2. Calculate the required number of normal samples for a 70/30 split\n",
    "# If 40k is 30%, then X is 70% -> (40,000 / 0.3) * 0.7\n",
    "num_normal_needed = int((len(df_anomaly) / 0.3) * 0.7)\n",
    "\n",
    "# 3. Downsample Class B (Normal)\n",
    "df_normal_downsampled = df_normal.sample(n=num_normal_needed, random_state=42)\n",
    "\n",
    "# 4. Combine them into your new training set\n",
    "df_train_7030 = pd.concat([df_anomaly, df_normal_downsampled])\n",
    "\n",
    "# 5. Shuffle the resulting dataframe\n",
    "df_train_7030 = df_train_7030.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Verification\n",
    "print(f\"New Training Set Size: {len(df_train_7030)}\")\n",
    "print(f\"Class 1 (Anomaly): {len(df_anomaly)} ({len(df_anomaly)/len(df_train_7030):.1%})\")\n",
    "print(f\"Class B (Normal): {len(df_normal_downsampled)} ({len(df_normal_downsampled)/len(df_train_7030):.1%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8a9ffb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_data_for_model(df, model_type, t_type='both', encoder = None, scaler = None):\n",
    "\n",
    "    X = df.drop(['anomaly', 'timestamp'], axis=1, errors='ignore')\n",
    "    y = df['anomaly']\n",
    "\n",
    "    cat_cols = ['building_month', 'building_hour', 'building_week_day_hour', 'primary_use']\n",
    "    \n",
    "    if model_type == \"svm\":\n",
    "        X = df.select_dtypes(include=['int64', 'float64'])\n",
    "        return X, y\n",
    "    \n",
    "    elif model_type == \"xgboost\":\n",
    "        if t_type == \"train\":\n",
    "            encoder = TargetEncoder(cols=cat_cols, smoothing=1.0)\n",
    "            X_encoded = encoder.fit_transform(X, y)\n",
    "            return X_encoded, y, encoder\n",
    "        elif t_type == \"test\":\n",
    "            X_encoded = encoder.transform(X)\n",
    "            return X_encoded, y\n",
    "        \n",
    "    elif model_type == \"knn\":\n",
    "        if t_type == \"train\":\n",
    "            encoder = TargetEncoder(cols=cat_cols, smoothing=1.0)\n",
    "            scaler = StandardScaler()\n",
    "            X_encoded = encoder.fit_transform(X, y)\n",
    "            X_scaled = scaler.fit_transform(X_encoded)\n",
    "            return X_scaled, y, encoder, scaler, X.columns.tolist()\n",
    "        elif t_type == \"test\":\n",
    "            X_encoded = encoder.transform(X)\n",
    "            X_scaled = scaler.transform(X_encoded)\n",
    "            return X_scaled, y\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported model type. Choose 'svm', 'xgboost', or 'knn'.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830cbc10",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e0e2584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.79      0.88    171008\n",
      "           1       0.04      0.44      0.08      3727\n",
      "\n",
      "    accuracy                           0.78    174735\n",
      "   macro avg       0.51      0.61      0.48    174735\n",
      "weighted avg       0.96      0.78      0.86    174735\n",
      "\n",
      "Accuracy Score: 0.7809\n",
      "ROC AUC Score: 0.6566\n"
     ]
    }
   ],
   "source": [
    "# 1. Prepare the data (handling the numeric selection as you did)\n",
    "X_train_svm, y_train_svm = prep_data_for_model(df_train_7030, \"svm\")\n",
    "X_test_svm, y_test_svm = prep_data_for_model(test_df, \"svm\")\n",
    "\n",
    "# 2. Define the Nystroem transformer \n",
    "# n_components=300 is a good balance between speed and accuracy. \n",
    "# Increasing this improves accuracy but slows down training.\n",
    "nystroem_stage = Nystroem(kernel='poly', degree=3, coef0=1, n_components=300, random_state=42)\n",
    "\n",
    "# 3. Use a fast linear solver\n",
    "# SGDClassifier is optimized for large datasets (100k+ rows)\n",
    "clf_stage = SGDClassifier(\n",
    "    loss='hinge', \n",
    "    class_weight='balanced',  # <--- THE MAGIC FIX\n",
    "    alpha=0.01, \n",
    "    max_iter=1000, \n",
    "    tol=1e-3,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 2. Re-build the same pipeline structure\n",
    "poly_svm_balanced = Pipeline([\n",
    "    ('scaler', preprocessing.StandardScaler()),\n",
    "    ('nystroem', nystroem_stage),\n",
    "    ('clf', clf_stage)\n",
    "])\n",
    "\n",
    "# 3. Re-train (This will still be fast!)\n",
    "poly_svm_balanced.fit(X_train_svm, y_train_svm)\n",
    "\n",
    "# 4. test\n",
    "y_pred = poly_svm_balanced.predict(X_test_svm)\n",
    "print(classification_report(y_test_svm, y_pred))\n",
    "\n",
    "print(f\"Accuracy Score: {accuracy_score(y_test_svm, y_pred):.4f}\")\n",
    "\n",
    "y_scores = poly_svm_balanced.decision_function(X_test_svm)\n",
    "print(f\"ROC AUC Score: {roc_auc_score(y_test_svm, y_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5830d856",
   "metadata": {},
   "source": [
    "## XG Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "49bf03a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_xgb, y_train_xgb, encoder_xgb = prep_data_for_model(df_train_7030, \"xgboost\", 'train')\n",
    "X_test_xgb, y_test_xgb = prep_data_for_model(test_df, \"xgboost\", 'test', encoder_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e57defbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters\n",
    "num_neg = (y_train_xgb == 0).sum()\n",
    "num_pos = (y_train_xgb == 1).sum()\n",
    "scale_weight = num_neg / num_pos\n",
    "\n",
    "xgb_params = { \\\n",
    "    'n_estimators' : 100,\n",
    "    'max_depth' : 6,\n",
    "    'learning_rate' : 0.1,\n",
    "    'scale_pos_weight' : scale_weight,\n",
    "    'tree_method' : 'hist',      \n",
    "    'enable_categorical' : True, \n",
    "    'random_state' : 42,\n",
    "    'use_label_encoder' : False,\n",
    "    'eval_metric' : 'logloss'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3950bd0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ferna\\Documents\\Repos\\e2e_ml_pipeline\\.venv\\Lib\\site-packages\\xgboost\\training.py:200: UserWarning: [23:24:41] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:782: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Training Complete!\n"
     ]
    }
   ],
   "source": [
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators= xgb_params['n_estimators'],\n",
    "    max_depth= xgb_params['max_depth'],\n",
    "    learning_rate= xgb_params['learning_rate'],\n",
    "    scale_pos_weight= xgb_params['scale_pos_weight'], # From our previous calculation\n",
    "    tree_method= xgb_params['tree_method'],      \n",
    "    enable_categorical= xgb_params['enable_categorical'], \n",
    "    random_state= xgb_params['random_state'],\n",
    "    use_label_encoder= xgb_params['use_label_encoder'],\n",
    "    eval_metric= xgb_params['eval_metric']\n",
    ")\n",
    "\n",
    "# 4. Fit the model\n",
    "# On 100k rows, this should take anywhere from 10 to 60 seconds\n",
    "xgb_model.fit(X_train_xgb, y_train_xgb)\n",
    "\n",
    "print(\"Model Training Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "635d5068",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_xgb = X_test_xgb\n",
    "y_pred_xgb = xgb_model.predict(X_test_xgb)\n",
    "y_probs_xgb = xgb_model.predict_proba(X_test_xgb)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7224fa9b",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1e82506a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_knn, y_train_knn, encoder_knn, scaler_knn, knn_cols = prep_data_for_model(df_train_7030, \"knn\", 'train')\n",
    "X_test_knn, y_test_knn = prep_data_for_model(test_df, \"knn\", 'test', encoder_knn, scaler_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e83dca06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.89\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.89      0.94    171008\n",
      "           1       0.14      0.82      0.24      3727\n",
      "\n",
      "    accuracy                           0.89    174735\n",
      "   macro avg       0.57      0.85      0.59    174735\n",
      "weighted avg       0.98      0.89      0.93    174735\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# 4. Initialize and train the model\n",
    "# Using k=5 as a starting point\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train_knn, y_train_knn)\n",
    "\n",
    "# 5. Predict and Evaluate\n",
    "y_pred = knn.predict(X_test_knn)\n",
    "\n",
    "print(f\"Accuracy: {accuracy_score(y_test_knn, y_pred):.2f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_knn, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee37d7c",
   "metadata": {},
   "source": [
    "# Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6eec573",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "92b90a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_svm_path_parquet = os.path.join(output_path_shap, 'svm/x_train.parquet')\n",
    "y_train_svm_path_parquet = os.path.join(output_path_shap, 'svm/y_train.parquet')\n",
    "x_test_svm_path_parquet = os.path.join(output_path_shap, 'svm/x_test.parquet')\n",
    "y_test_svm_path_parquet = os.path.join(output_path_shap, 'svm/y_test.parquet')\n",
    "\n",
    "joblib.dump(poly_svm_balanced, output_path + \"svm/model.pkl\")\n",
    "\n",
    "X_train_svm.to_parquet(x_train_svm_path_parquet, index=False)\n",
    "y_train_svm.to_frame(name='anomaly').to_parquet(y_train_svm_path_parquet, index=False)\n",
    "X_test_svm.to_parquet(x_test_svm_path_parquet, index=False)\n",
    "y_test_svm.to_frame(name='anomaly').to_parquet(y_test_svm_path_parquet, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9957e1",
   "metadata": {},
   "source": [
    "## XG-Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "321ac8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_xgb_path_parquet = os.path.join(output_path_shap, 'xgb/x_train.parquet')\n",
    "y_train_xgb_path_parquet = os.path.join(output_path_shap, 'xgb/y_train.parquet')\n",
    "x_test_xgb_path_parquet = os.path.join(output_path_shap, 'xgb/x_test.parquet')\n",
    "y_test_xgb_path_parquet = os.path.join(output_path_shap, 'xgb/y_test.parquet')\n",
    "\n",
    "joblib.dump(xgb_model, output_path + \"xgb/model.pkl\")\n",
    "\n",
    "X_train_xgb.to_parquet(x_train_xgb_path_parquet, index=False)\n",
    "y_train_xgb.to_frame(name='anomaly').to_parquet(y_train_xgb_path_parquet, index=False)\n",
    "X_test_xgb.to_parquet(x_test_xgb_path_parquet, index=False)\n",
    "y_test_xgb.to_frame(name='anomaly').to_parquet(y_test_xgb_path_parquet, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec655fd",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "95e29fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_knn_path_parquet = os.path.join(output_path_shap, 'knn/x_train.parquet')\n",
    "y_train_knn_path_parquet = os.path.join(output_path_shap, 'knn/y_train.parquet')\n",
    "x_test_knn_path_parquet = os.path.join(output_path_shap, 'knn/x_test.parquet')\n",
    "y_test_knn_path_parquet = os.path.join(output_path_shap, 'knn/y_test.parquet')\n",
    "\n",
    "joblib.dump(knn, output_path + \"knn/model.pkl\")\n",
    "\n",
    "X_train_knn_df = pd.DataFrame(X_train_knn, columns = knn_cols )\n",
    "X_train_knn_df.to_parquet(x_train_knn_path_parquet, index=False)\n",
    "\n",
    "y_train_knn_df = pd.DataFrame(y_train_knn, columns=['anomaly'])\n",
    "y_train_knn_df.to_parquet(y_train_knn_path_parquet, index=False)\n",
    "\n",
    "X_test_knn_df = pd.DataFrame(X_test_knn, columns = knn_cols)\n",
    "X_test_knn_df.to_parquet(x_test_knn_path_parquet, index=False)\n",
    "\n",
    "y_test_knn_df = pd.DataFrame(y_test_knn, columns=['anomaly'])\n",
    "y_test_knn_df.to_parquet(y_test_knn_path_parquet, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
